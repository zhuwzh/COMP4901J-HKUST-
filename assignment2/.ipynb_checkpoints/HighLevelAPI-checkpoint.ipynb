{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 10; D = 2;\n",
    "X_train = np.random.randn(N,D)\n",
    "y_train = np.random.randn(N,1) + 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.07176383  1.55206552]\n",
      " [ 0.36465616 -0.61776793]\n",
      " [-0.6836818   1.14966895]\n",
      " [-0.32946404 -0.35177542]\n",
      " [-1.47460342  4.18402927]\n",
      " [ 0.53205103 -0.70108246]\n",
      " [-0.98566574 -0.20508708]\n",
      " [-0.10367773  2.30893864]\n",
      " [ 0.01786541  0.20326622]\n",
      " [-1.08722972 -1.23466882]]\n",
      "[[  9.2496369 ]\n",
      " [ 10.38015686]\n",
      " [  8.21272766]\n",
      " [  9.7562016 ]\n",
      " [ 10.04225355]\n",
      " [  9.2610251 ]\n",
      " [  8.9879274 ]\n",
      " [  9.63810551]\n",
      " [ 10.04395015]\n",
      " [ 12.02433359]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(dtype=tf.float32, shape=[10,2], name='X')\n",
    "y = tf.placeholder(dtype=tf.float32, shape=[10,1], name='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('FC'):\n",
    "    f_1 = layers.fully_connected(inputs=X,\n",
    "                                 num_outputs=128,\n",
    "                                 scope='l_1')\n",
    "    f_2 = layers.fully_connected(inputs=X,\n",
    "                                 num_outputs=1,\n",
    "                                 activation_fn=None,\n",
    "                                 scope='l_2',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (<ipython-input-168-2f79e721292f>, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-168-2f79e721292f>\"\u001b[0;36m, line \u001b[0;32m13\u001b[0m\n\u001b[0;31m    s)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "loss = tf.losses.mean_squared_error(labels=y,predictions=f_2,scope='loss')\n",
    "global_step=tf.Variable(initial_value=tf.cast(1e4,tf.int32),\n",
    "                        trainable=False,\n",
    "                        name='global_step')\n",
    "optimizer = lambda lr: tf.train.RMSPropOptimizer(learning_rate=lr,\n",
    "                                                 decay=.99,\n",
    "                                                 momentum=0.0)\n",
    "def exp_decay(learning_rate, global_step):\n",
    "    return tf.train.exponential_decay(learning_rate= learning_rate,\n",
    "                                      global_step=global_step,\n",
    "                                      decay_steps=10000,\n",
    "                                      decay_rate=.9,\n",
    "                                      s)\n",
    "\n",
    "\n",
    "train_op = layers.optimize_loss(loss=loss,\n",
    "                                global_step=global_step,\n",
    "                                learning_rate=1e-1,\n",
    "                                learning_rate_decay_fn=exp_decay,\n",
    "                                optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.873\n",
      "0.765194\n",
      "0.765179\n",
      "0.765177\n",
      "0.765177\n",
      "0.765177\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for iter in range(30000):    \n",
    "    train_loss,_ = sess.run([loss,train_op],\n",
    "                            feed_dict={X:X_train, y:y_train})\n",
    "    if iter % 5000 == 0:\n",
    "        print(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable_5:0' shape=() dtype=float32_ref>"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = tf.Variable(initial_value=1e-2,trainable=False,dtype=tf.float32)\n",
    "learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.mean_squared_error(labels=y,predictions=f_2,scope='loss')\n",
    "\n",
    "lr = tf.Variable(initial_value=1e-2,trainable=False,dtype=tf.float32)\n",
    "\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=lr,\n",
    "                                      decay=.99,\n",
    "                                      momentum=0.0)\n",
    "learning_rate=1e-2; lr_decay = .9\n",
    "\n",
    "train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.765358\n",
      "0.765358\n",
      "0.765358\n",
      "Decay with lr 0.009000\n",
      "0.765324\n",
      "Decay with lr 0.008100\n",
      "0.765296\n",
      "Decay with lr 0.007290\n",
      "0.765273\n",
      "Decay with lr 0.006561\n",
      "0.765255\n",
      "Decay with lr 0.005905\n",
      "0.76524\n",
      "Decay with lr 0.005314\n",
      "0.765228\n",
      "Decay with lr 0.004783\n",
      "0.765218\n",
      "Decay with lr 0.004305\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "accumulate_loss = []\n",
    "for ep in range(10):\n",
    "    for it in range(10000):\n",
    "            train_loss,_ = sess.run([loss,train_op],\n",
    "                            feed_dict={X:X_train, y:y_train, lr:learning_rate})\n",
    "    print(train_loss)\n",
    "    accumulate_loss.append(train_loss)\n",
    "    if ep>1 and accumulate_loss[ep]/accumulate_loss[ep-1] > .9:\n",
    "        learning_rate *= lr_decay\n",
    "        print('Decay with lr %f' % learning_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
