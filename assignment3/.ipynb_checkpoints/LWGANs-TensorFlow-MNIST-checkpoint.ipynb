{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Networks (GANs)\n",
    "\n",
    "So far in CS231N, all the applications of neural networks that we have explored have been **discriminative models** that take an input and are trained to produce a labeled output. This has ranged from straightforward classification of image categories to sentence generation (which was still phrased as a classification problem, our labels were in vocabulary space and weâ€™d learned a recurrence to capture multi-word labels). In this notebook, we will expand our repetoire, and build **generative models** using neural networks. Specifically, we will learn how to build models which generate novel images that resemble a set of training images.\n",
    "\n",
    "### What is a GAN?\n",
    "\n",
    "In 2014, [Goodfellow et al.](https://arxiv.org/abs/1406.2661) presented a method for training generative models called Generative Adversarial Networks (GANs for short). In a GAN, we build two different neural networks. Our first network is a traditional classification network, called the **discriminator**. We will train the discriminator to take images, and classify them as being real (belonging to the training set) or fake (not present in the training set). Our other network, called the **generator**, will take random noise as input and transform it using a neural network to produce images. The goal of the generator is to fool the discriminator into thinking the images it produced are real.\n",
    "\n",
    "We can think of this back and forth process of the generator ($G$) trying to fool the discriminator ($D$), and the discriminator trying to correctly classify real vs. fake as a minimax game:\n",
    "$$\\underset{G}{\\text{minimize}}\\; \\underset{D}{\\text{maximize}}\\; \\mathbb{E}_{x \\sim p_\\text{data}}\\left[\\log D(x)\\right] + \\mathbb{E}_{z \\sim p(z)}\\left[\\log \\left(1-D(G(z))\\right)\\right]$$\n",
    "where $x \\sim p_\\text{data}$ are samples from the input data, $z \\sim p(z)$ are the random noise samples, $G(z)$ are the generated images using the neural network generator $G$, and $D$ is the output of the discriminator, specifying the probability of an input being real. In [Goodfellow et al.](https://arxiv.org/abs/1406.2661), they analyze this minimax game and show how it relates to minimizing the Jensen-Shannon divergence between the training data distribution and the generated samples from $G$.\n",
    "\n",
    "To optimize this minimax game, we will aternate between taking gradient *descent* steps on the objective for $G$, and gradient *ascent* steps on the objective for $D$:\n",
    "1. update the **generator** ($G$) to minimize the probability of the __discriminator making the correct choice__. \n",
    "2. update the **discriminator** ($D$) to maximize the probability of the __discriminator making the correct choice__.\n",
    "\n",
    "While these updates are useful for analysis, they do not perform well in practice. Instead, we will use a different objective when we update the generator: maximize the probability of the **discriminator making the incorrect choice**. This small change helps to allevaiate problems with the generator gradient vanishing when the discriminator is confident. This is the standard update used in most GAN papers, and was used in the original paper from [Goodfellow et al.](https://arxiv.org/abs/1406.2661). \n",
    "\n",
    "In this assignment, we will alternate the following updates:\n",
    "1. Update the generator ($G$) to maximize the probability of the discriminator making the incorrect choice on generated data:\n",
    "$$\\underset{G}{\\text{maximize}}\\;  \\mathbb{E}_{z \\sim p(z)}\\left[\\log D(G(z))\\right]$$\n",
    "2. Update the discriminator ($D$), to maximize the probability of the discriminator making the correct choice on real and generated data:\n",
    "$$\\underset{D}{\\text{maximize}}\\; \\mathbb{E}_{x \\sim p_\\text{data}}\\left[\\log D(x)\\right] + \\mathbb{E}_{z \\sim p(z)}\\left[\\log \\left(1-D(G(z))\\right)\\right]$$\n",
    "\n",
    "### What else is there?\n",
    "Since 2014, GANs have exploded into a huge research area, with massive [workshops](https://sites.google.com/site/nips2016adversarial/), and [hundreds of new papers](https://github.com/hindupuravinash/the-gan-zoo). Compared to other approaches for generative models, they often produce the highest quality samples but are some of the most difficult and finicky models to train (see [this github repo](https://github.com/soumith/ganhacks) that contains a set of 17 hacks that are useful for getting models working). Improving the stabiilty and robustness of GAN training is an open research question, with new papers coming out every day! For a more recent tutorial on GANs, see [here](https://arxiv.org/abs/1701.00160). There is also some even more recent exciting work that changes the objective function to Wasserstein distance and yields much more stable results across model architectures: [WGAN](https://arxiv.org/abs/1701.07875), [WGAN-GP](https://arxiv.org/abs/1704.00028).\n",
    "\n",
    "\n",
    "GANs are not the only way to train a generative model! For other approaches to generative modeling check out the [deep generative model chapter](http://www.deeplearningbook.org/contents/generative_models.html) of the Deep Learning [book](http://www.deeplearningbook.org). Another popular way of training neural networks as generative models is Variational Autoencoders (co-discovered [here](https://arxiv.org/abs/1312.6114) and [here](https://arxiv.org/abs/1401.4082)). Variational autoencoders combine neural networks with variational inference to train deep generative models. These models tend to be far more stable and easier to train but currently don't produce samples that are as pretty as GANs.\n",
    "\n",
    "Example pictures of what you should expect (yours might look slightly different):\n",
    "\n",
    "![caption](gan_outputs_tf.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow.contrib.layers as ly\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# A bunch of utility functions\n",
    "\n",
    "def show_images(images):\n",
    "    images = np.reshape(images, [images.shape[0], -1])  # images reshape to (batch_size, D)\n",
    "    sqrtn = int(np.ceil(np.sqrt(images.shape[0])))\n",
    "    sqrtimg = int(np.ceil(np.sqrt(images.shape[1])))\n",
    "\n",
    "    fig = plt.figure(figsize=(sqrtn, sqrtn))\n",
    "    gs = gridspec.GridSpec(sqrtn, sqrtn)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, img in enumerate(images):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(img.reshape([sqrtimg,sqrtimg]))\n",
    "    return\n",
    "\n",
    "def preprocess_img(x):\n",
    "    return 2 * x - 1.0\n",
    "\n",
    "def deprocess_img(x):\n",
    "    return (x + 1.0) / 2.0\n",
    "\n",
    "def rel_error(x,y):\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "def count_params():\n",
    "    \"\"\"Count the number of parameters in the current TensorFlow graph \"\"\"\n",
    "    param_count = np.sum([np.prod(x.get_shape().as_list()) for x in tf.global_variables()])\n",
    "    return param_count\n",
    "\n",
    "\n",
    "def get_session():\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    session = tf.Session(config=config)\n",
    "    return session\n",
    "\n",
    "answers = np.load('gan-checks-tf.npz')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./cs231n/datasets/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ./cs231n/datasets/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./cs231n/datasets/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./cs231n/datasets/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPMAAADuCAYAAADsvjF6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZfoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a15d7a2b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('./cs231n/datasets/MNIST_data', one_hot=False)\n",
    "\n",
    "# show a batch\n",
    "show_images(mnist.train.next_batch(16)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_noise(batch_size, dim):\n",
    "    \"\"\"Generate random normal noise\n",
    "    \n",
    "    Inputs:\n",
    "    - batch_size: integer giving the batch size of noise to generate\n",
    "    - dim: integer giving the dimension of the the noise to generate\n",
    "    \n",
    "    Returns:\n",
    "    TensorFlow Tensor containing uniform noise in [-1, 1] with shape [batch_size, dim]\n",
    "    \"\"\"\n",
    "    # TODO: sample and return noise\n",
    "    return tf.random_normal(shape=[batch_size, dim],\n",
    "                            mean=0,\n",
    "                            stddev=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def leaky_relu(x, alpha=.1):\n",
    "    \"\"\"Compute the leaky ReLU activation function.\n",
    "    \n",
    "    Inputs:\n",
    "    - x: TensorFlow Tensor with arbitrary shape\n",
    "    - alpha: leak parameter for leaky ReLU\n",
    "    \n",
    "    Returns:\n",
    "    TensorFlow Tensor with the same shape as x\n",
    "    \"\"\"\n",
    "    # TODO: implement leaky ReLU    \n",
    "    \n",
    "    return tf.maximum(x,0) + tf.minimum(x,0) * alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def discriminator(x):\n",
    "    '''\n",
    "    Input : x : [batch_size, 784]\n",
    "    Output : logits : [batch_size , 1]\n",
    "    '''\n",
    "    x = tf.reshape(x, [-1,28,28,1])\n",
    "    \n",
    "    with tf.variable_scope('discriminator'):\n",
    "        \n",
    "        conv_1 = ly.conv2d(inputs=x, # 28 * 28 * 8 \n",
    "                           num_outputs=8,\n",
    "                           kernel_size=1,\n",
    "                           stride=1,\n",
    "                           padding='SAME',\n",
    "                           activation_fn = leaky_relu,\n",
    "                           normalizer_fn = ly.batch_norm,\n",
    "                           scope='conv_1')\n",
    "                           #weights_initializer = tf.random_normal_initializer(0,0.02))\n",
    "        conv_2 = ly.conv2d(inputs=conv_1, # 14 * 14 * 16 \n",
    "                           num_outputs=16,\n",
    "                           kernel_size=3,\n",
    "                           stride=2,\n",
    "                           padding='SAME',\n",
    "                           activation_fn = leaky_relu,\n",
    "                           normalizer_fn = ly.batch_norm,\n",
    "                           scope='conv_2')\n",
    "                           #weights_initializer = tf.random_normal_initializer(0,0.02))\n",
    "        conv_3 = ly.conv2d(inputs=conv_2, # 7 * 7 * 128\n",
    "                           num_outputs=32,\n",
    "                           kernel_size=3,\n",
    "                           stride=2,\n",
    "                           padding='SAME',\n",
    "                           activation_fn = leaky_relu,\n",
    "                           normalizer_fn = ly.batch_norm,\n",
    "                           scope='conv_3')\n",
    "                           #weights_initializer = tf.random_normal_initializer(0,0.02))\n",
    "        conv_4 = ly.conv2d(inputs=conv_3, # 4 * 4 * 64 \n",
    "                           num_outputs=64,\n",
    "                           kernel_size=3,\n",
    "                           stride=2,\n",
    "                           padding='SAME',\n",
    "                           activation_fn = leaky_relu,\n",
    "                           normalizer_fn = ly.batch_norm,\n",
    "                           scope='conv_4')\n",
    "                           #weights_initializer = tf.random_normal_initializer(0,0.02))\n",
    "        conv_4 = tf.reshape(conv_4, [-1,4*4*64])\n",
    "        logits = ly.fully_connected(inputs=conv_4, \n",
    "                                    num_outputs=1,\n",
    "                                    weights_initializer = tf.random_uniform_initializer(0,.02),\n",
    "                                    scope='logits')        \n",
    "        #print(logits.shape)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf.reset_default_graph()\n",
    "# x = tf.random_normal(shape=[10,784])\n",
    "# with tf.Session() as sess:\n",
    "#     a = discriminator(x)\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "#     sess.run(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator : DConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(z1,z2,z3):\n",
    "    \"\"\"Generate images from a random noise vector.\n",
    "    \n",
    "    Inputs:\n",
    "    - z: TensorFlow Tensor of random noise with shape [batch_size, noise_dim]\n",
    "    \n",
    "    Returns:\n",
    "    TensorFlow Tensor of generated images, with shape [batch_size, 784].\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(\"generator\"):\n",
    "\n",
    "        fc_z1 = ly.fully_connected(inputs=z1, \n",
    "                                   num_outputs=3 * 3 * 64,\n",
    "                                   activation_fn = leaky_relu,\n",
    "                                   normalizer_fn = ly.batch_norm,\n",
    "                                   weights_initializer = tf.random_uniform_initializer(0,.02),\n",
    "                                   scope='fc_z1')\n",
    "        fc_z1_reshape = tf.reshape(fc_z1, shape=[-1,3,3,64]) # 3 * 3 * 64\n",
    "        dconv_1 = ly.conv2d_transpose(inputs=fc_z1_reshape, # 7 * 7 * 32\n",
    "                                      num_outputs=32,\n",
    "                                      kernel_size=3,\n",
    "                                      stride=2,\n",
    "                                      padding='VALID',\n",
    "                                      activation_fn=leaky_relu,\n",
    "                                      normalizer_fn=ly.batch_norm,\n",
    "                                      weights_initializer= tf.random_normal_initializer(0,.02),\n",
    "                                      scope='dconv_1') \n",
    "        \n",
    "        fc_z2 = ly.fully_connected(inputs=z2, \n",
    "                                   num_outputs=7 * 7 * 32,\n",
    "                                   activation_fn = leaky_relu,\n",
    "                                   normalizer_fn = ly.batch_norm,\n",
    "                                   weights_initializer = tf.random_uniform_initializer(0,.02),\n",
    "                                   scope='fc_z2') \n",
    "        fc_z2_reshape = tf.reshape(fc_z2, shape=[-1,7,7,32])\n",
    "        merge_1 = tf.add(fc_z2_reshape , dconv_1, name='merge_1') # 7 * 7 * 32\n",
    "        dconv_2 = ly.conv2d_transpose(inputs=merge_1, # 14 * 14 * 16\n",
    "                                      num_outputs=16,\n",
    "                                      kernel_size=3,\n",
    "                                      stride=2,\n",
    "                                      padding='SAME',\n",
    "                                      activation_fn=leaky_relu,\n",
    "                                      normalizer_fn=ly.batch_norm,\n",
    "                                      weights_initializer= tf.random_normal_initializer(0,.02),\n",
    "                                      scope='dconv_2')\n",
    "        \n",
    "        fc_z3 = ly.fully_connected(inputs=z3, \n",
    "                                   num_outputs=14 * 14 * 16,\n",
    "                                   activation_fn = leaky_relu,\n",
    "                                   normalizer_fn = ly.batch_norm,\n",
    "                                   weights_initializer = tf.random_uniform_initializer(0,.02),\n",
    "                                   scope='fc_z3')\n",
    "        fc_z3_reshape = tf.reshape(fc_z3, shape=[-1,14,14,16])\n",
    "        merge_2 = tf.add(fc_z3_reshape , dconv_2, name='merge_2')\n",
    "        \n",
    "        dconv_3 = ly.conv2d_transpose(inputs=merge_2, # 28 * 28 * 8\n",
    "                                      num_outputs=8,\n",
    "                                      kernel_size=3,\n",
    "                                      stride=2,\n",
    "                                      padding='SAME',\n",
    "                                      activation_fn=leaky_relu,\n",
    "                                      normalizer_fn=ly.batch_norm,\n",
    "                                      weights_initializer= tf.random_normal_initializer(0,.02),\n",
    "                                      scope='dconv_3')\n",
    "        img = ly.conv2d_transpose(inputs=dconv_3, # 28 * 28 * 1\n",
    "                                  num_outputs=1,\n",
    "                                  kernel_size=3,\n",
    "                                  stride=1,\n",
    "                                  padding='SAME',\n",
    "                                  weights_initializer= tf.random_normal_initializer(0,.02),\n",
    "                                  scope='img')                                 \n",
    "        img = tf.reshape(img,[-1,784]) # 784\n",
    "        #print(img.shape)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 784)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    z1 = tf.random_normal([10,16])\n",
    "    z2 = tf.random_normal([10,20])\n",
    "    z3 = tf.random_normal([10,8])\n",
    "    a = generator(z1,z2,z3)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WGANs Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wgangp_loss(logits_real, logits_fake):\n",
    "    \"\"\"Compute the WGAN-GP loss.\n",
    "    \n",
    "    Inputs:\n",
    "    - logits_real: Tensor, shape [batch_size, 1], output of discriminator\n",
    "    - logits_fake: Tensor, shape[batch_size, 1], output of discriminator\n",
    "    - batch_size: The number of examples in this batch\n",
    "    - x: the input (real) images for this batch\n",
    "    - G_sample: the generated (fake) images for this batch\n",
    "    \n",
    "    Returns:\n",
    "    - D_loss: discriminator loss scalar\n",
    "    - G_loss: generator loss scalar\n",
    "    \"\"\"\n",
    "    # TODO: compute D_loss and G_loss\n",
    "    D_loss =  - tf.reduce_mean(logits_real - logits_fake)\n",
    "    \n",
    "    G_loss =  - tf.reduce_mean(logits_fake)\n",
    "\n",
    "    return D_loss, G_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_solvers(learning_rate=5e-5, beta1=.5, beta2=.9):\n",
    "    \"\"\"Create solvers for WGAN training.\n",
    "    \n",
    "    Inputs:\n",
    "    - learning_rate: learning rate to use for both solvers\n",
    "    - beta1: beta1 parameter for both solvers (first moment decay)\n",
    "    \n",
    "    Returns:\n",
    "    - D_solver: instance of tf.train.AdamOptimizer with correct learning_rate and beta1\n",
    "    - G_solver: instance of tf.train.AdamOptimizer with correct learning_rate and beta1\n",
    "    \"\"\"\n",
    "    D_solver = tf.train.AdamOptimizer(learning_rate = learning_rate,\n",
    "                                      beta1=beta1, beta2=beta2)\n",
    "                                         \n",
    "    \n",
    "    G_solver = tf.train.AdamOptimizer(learning_rate = learning_rate,\n",
    "                                      beta1=beta1, beta2=beta2)\n",
    "    return D_solver, G_solver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# learning rate of two networks\n",
    "learning_rate_gen = 5e-5\n",
    "learning_rate_dis = 5e-5\n",
    "\n",
    "# steps of dis per global step\n",
    "citers = 5\n",
    "\n",
    "# batch_size\n",
    "batch_size = 64\n",
    "\n",
    "# noise_dim\n",
    "z1_dim = 3 * 3\n",
    "z2_dim = 5 * 5\n",
    "z3_dim = 7 * 7\n",
    "\n",
    "\n",
    "# leaky relu paramter\n",
    "alpha = .1\n",
    "\n",
    "# maxiter = train_samples * epoch / batch_size\n",
    "epoch = 50\n",
    "print_every = 100\n",
    "show_every = 500\n",
    "\n",
    "# Adam optimizer \n",
    "beta1 = .5\n",
    "beta2 = .9\n",
    "\n",
    "# restriction on weights of discriminator network\n",
    "clamp_lower = -.01\n",
    "clamp_upper = .01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# placeholders for images from the training dataset\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "z1 = sample_noise(batch_size=batch_size, dim=z1_dim)\n",
    "z2 = sample_noise(batch_size=batch_size, dim=z2_dim)\n",
    "z3 = sample_noise(batch_size=batch_size, dim=z3_dim)\n",
    "# generated images\n",
    "G_sample = generator(z1,z2,z3)\n",
    "\n",
    "with tf.variable_scope(\"\") as scope:\n",
    "    #scale images to be -1 to 1\n",
    "    logits_real = discriminator(preprocess_img(x))\n",
    "    # Re-use discriminator weights on new inputs, share parameters\n",
    "    scope.reuse_variables()\n",
    "    logits_fake = discriminator(G_sample)\n",
    "\n",
    "# Get the list of variables for the discriminator and generator\n",
    "D_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,'discriminator')\n",
    "D_clip = [v.assign(tf.clip_by_value(v, clamp_lower, clamp_upper)) for v in D_vars]\n",
    "\n",
    "G_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,'generator')\n",
    "\n",
    "D_solver, G_solver = get_solvers()\n",
    "\n",
    "D_loss, G_loss = wgangp_loss(logits_real, logits_fake)\n",
    "D_train_step = D_solver.minimize(D_loss, var_list=D_vars)\n",
    "G_train_step = G_solver.minimize(G_loss, var_list=G_vars)\n",
    "\n",
    "with tf.control_dependencies([D_train_step]):\n",
    "    D_train_step = tf.tuple(D_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a giant helper function\n",
    "def run_a_gan(sess, G_train_step, G_loss, D_train_step, D_loss,\\\n",
    "              show_every=show_every, print_every=print_every,\\\n",
    "              Citers = citers, batch_size=batch_size, num_epoch=epoch):\n",
    "    \"\"\"Train a GAN for a certain number of epochs.\n",
    "    \n",
    "    Inputs:\n",
    "    - sess: A tf.Session that we want to use to run our data\n",
    "    - Citers : discriminator steps per global step\n",
    "    - G_train_step: A training step for the Generator\n",
    "    - G_loss: Generator loss\n",
    "    - D_train_step: A training step for the Generator\n",
    "    - D_loss: Discriminator loss\n",
    "    Returns:\n",
    "        Nothing\n",
    "    \"\"\"\n",
    "    # compute the number of iterations we need\n",
    "    max_iter = int(mnist.train.num_examples*num_epoch/batch_size)\n",
    "    for it in range(max_iter):\n",
    "        ## every show often, show a sample result\n",
    "        if it % show_every == 0:\n",
    "            samples = sess.run(G_sample)\n",
    "            fig = show_images(samples[:16])\n",
    "            plt.show()\n",
    "            print()\n",
    "        ## setting citers per global step    \n",
    "        if it < 50 or it % 500 == 0:\n",
    "            citers = 50\n",
    "        else:\n",
    "            citers = Citers\n",
    "            \n",
    "        for ci in range(citers):\n",
    "            # update discriminators citers times\n",
    "            minibatch, _ = mnist.train.next_batch(batch_size)\n",
    "            if ci != citers - 1:\n",
    "                sess.run(D_train_step, feed_dict={x:minibatch})\n",
    "            elif ci == citers - 1: \n",
    "                _, D_loss_curr = sess.run([D_train_step, D_loss], feed_dict={x:minibatch})\n",
    "                    \n",
    "        # update generator every global step    \n",
    "        _, G_loss_curr = sess.run([G_train_step, G_loss], feed_dict={x: minibatch})\n",
    "\n",
    "        # print loss every so often.\n",
    "        # We want to make sure D_loss doesn't go to 0\n",
    "        if it % print_every == 0:\n",
    "            print('Iter: {}, D: {:.4}, G:{:.4}'.format(it,D_loss_curr,G_loss_curr))\n",
    "    print('Final images')\n",
    "    samples = sess.run(G_sample)\n",
    "\n",
    "    fig = show_images(samples[:16])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPMAAADuCAYAAADsvjF6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a281850b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iter: 0, D: -3.152, G:-0.06737\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-4fbe7b758b5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mrun_a_gan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mG_train_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mG_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mD_train_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mD_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-27-654e2cd8c465>\u001b[0m in \u001b[0;36mrun_a_gan\u001b[0;34m(sess, G_train_step, G_loss, D_train_step, D_loss, show_every, print_every, Citers, batch_size, num_epoch)\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mminibatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mci\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mciters\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                 \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_train_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mci\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mciters\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_loss_curr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mD_train_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zwz/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zwz/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zwz/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zwz/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zwz/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with get_session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    run_a_gan(sess,G_train_step,G_loss,D_train_step,D_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
